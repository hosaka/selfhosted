version: "3"

x-logging: &logging
  driver: json-file
  options:
    max-size: 100m
    max-file: "3"
    tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"

x-hotio: &environment-hotio
  environment:
    - PUID=${MEDIA_PUID}
    - PGID=${MEDIA_PGID}
    - UMASK=${MEDIA_UMASK}
    - TZ=${TIMEZONE}

networks:
  proxy:
  cloudflared:
  qbit:
  immich:
  paperless:
  monitor:
  mqtt:
  forgejo:

volumes:
  caddy-config:

  jellyfin-config:
  radarr-config:
  sonarr-config:
  lidarr-config:
  navidrome-data:
  prowlarr-config:
  recyclarr-config:
  jellyseerr-config:
  sabnzbd-config:
  qbit-config:
  qbitmanage-config:

  immich-model-cache:
  immich-redis-data:
  immich-database-data:

  paperless-redis-data:
  paperless-data:
  paperless-media:

  vaultwarden-data:

  home-assistant-config:
  mosquitto-data:
  forgejo-data:
  forgejo-database-data:

  # monitoring
  gotify-data:
  diun-data:
  grafana-data:
  grafana-config:
  prometheus-data:

services:
  caddy:
    image: docker.io/hotio/caddy:release-2.7.6
    environment:
      - PROXY_CF_API_EMAIL=${PROXY_CF_API_EMAIL}
      - PROXY_CF_API_TOKEN=${PROXY_CF_API_TOKEN}
      - PROXY_PUBLIC_DOMAIN=${PROXY_PUBLIC_DOMAIN}
      - PROXY_PRIVATE_DOMAIN=${PROXY_PRIVATE_DOMAIN}
      - CADDY_INGRESS_NETWORKS=proxy
      - PUID=${MEDIA_PUID}
      - PGID=${MEDIA_PGID}
      - UMASK=${MEDIA_UMASK}
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
      - cloudflared
    ports:
      - 80:80
      - 443:443/tcp
      - 443:443/udp
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - caddy-config:/config
      - ./caddy/Caddyfile:/config/Caddyfile

  cloudflared:
    image: docker.io/cloudflare/cloudflared:2024.1.4
    command: tunnel --no-autoupdate run --token ${PROXY_CF_TUNNEL_TOKEN}
    extra_hosts:
      - host.docker.internal:host-gateway
    # expose:
    #   - 4506 # metrics
    logging: *logging
    networks:
      - cloudflared
      # - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro

  jellyfin:
    image: docker.io/hotio/jellyfin:release-10.8.13-1
    devices:
      - ${MEDIA_JELLYFIN_CARD}:/dev/dri/card0
      - ${MEDIA_JELLYFIN_RENDER}:/dev/dri/renderD128
    <<: *environment-hotio
    expose:
      - 8096 # webui
    group_add:
      - ${MEDIA_JELLYFIN_VIDEO_GROUP}
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_COLLECTION_PATH}:/data/media/collection
      - jellyfin-config:/config

  jellyseerr:
    image: docker.io/hotio/jellyseerr:release-1.7.0
    <<: *environment-hotio
    expose:
      - 5055 # webui
    logging: *logging
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - jellyseerr-config:/app/config

  radarr:
    image: docker.io/hotio/radarr:release-5.2.6.8376
    <<: *environment-hotio
    expose:
      - 7878 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - radarr-config:/config

  sonarr:
    image: docker.io/hotio/sonarr:release-4.0.1.929
    <<: *environment-hotio
    expose:
      - 8989 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - sonarr-config:/config

  prowlarr:
    image: docker.io/hotio/prowlarr:release-1.12.2.4211
    <<: *environment-hotio
    expose:
      - 9696 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - prowlarr-config:/config

  recyclarr:
    image: docker.io/recyclarr/recyclarr:6.0.2
    environment:
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - recyclarr-config:/config
      - ./recyclarr/recyclarr.yml:/config/recyclarr.yml

  navidrome:
    image: docker.io/deluan/navidrome:0.51.0
    environment:
      - ND_ENABLETRANSCODINGCONFIG=false
    expose:
      - 4533 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - navidrome-data:/data
      - ${MEDIA_COLLECTION_MUSIC_PATH}:/music:ro

  lidarr:
    image: docker.io/hotio/lidarr:release-2.0.7.3849
    <<: *environment-hotio
    expose:
      - 8686 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - lidarr-config:/config
      - ${MEDIA_ROOT_PATH}:/data/media

  sabnzbd:
    image: docker.io/hotio/sabnzbd:release-4.2.1
    <<: *environment-hotio
    expose:
      - 8080 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - sabnzbd-config:/config
      - ${MEDIA_DOWNLOADS_USENET_PATH}:/data/media/downloads/usenet

  qbit:
    image: docker.io/hotio/qbittorrent:release
    cap_add:
      - NET_ADMIN
    environment:
      - PUID=${MEDIA_PUID}
      - PGID=${MEDIA_PGID}
      - UMASK=${MEDIA_UMASK}
      - TZ=${TIMEZONE}
      - VPN_ENABLED=${MEDIA_TORRENT_VPN_ENABLED}
      - VPN_IP_CHECK_DELAY=${MEDIA_TORRENT_VPN_IP_CHECK_DELAY}
      - VPN_IP_CHECK_EXIT=${MEDIA_TORRENT_VPN_IP_CHECK_EXIT}
      - VPN_LAN_NETWORK=${MEDIA_TORRENT_VPN_LAN_NETWORK}
    expose:
      - 8080 # webui
    logging: *logging
    networks:
      - proxy
      - qbit
    restart: unless-stopped
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=1
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - qbit-config:/config
      - ./qbittorrent/wg0.conf:/config/wireguard/wg0.conf:ro
      - ${MEDIA_DOWNLOADS_TORRENT_PATH}:/data/media/downloads/torrent

  qbitmanage:
    image: docker.io/hotio/qbitmanage:release
    <<: *environment-hotio
    logging: *logging
    networks:
      - qbit
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - qbitmanage-config:/config
      - ./qbittorrent/qbitmanage.yml:/config/config.yml
      - ${MEDIA_DOWNLOADS_TORRENT_PATH}:/data/media/downloads/torrent

  immich:
    image: docker.io/altran1502/immich-server:release
    depends_on:
      - immich-redis
      - immich-database
    entrypoint: ["/bin/sh", "./start-server.sh"]
    environment:
      - NODE_ENV=production
      - DB_HOSTNAME=immich-database
      - DB_USERNAME=${IMMICH_DB_USERNAME}
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - DB_DATABASE_NAME=${IMMICH_DB_DATABASE_NAME}
      - REDIS_HOSTNAME=immich-redis
      - JWT_SECRET=${IMMICH_JWT_SECRET}
      - LOG_LEVEL=${IMMICH_LOG_LEVEL}
      - TZ=${TIMEZONE}
    expose:
      - 3001 # webui
    logging: *logging
    networks:
      - proxy
      - immich
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-microservices:
    image: docker.io/altran1502/immich-server:release
    depends_on:
      - immich-redis
      - immich-database
    entrypoint: ["/bin/sh", "./start-microservices.sh"]
    environment:
      - NODE_ENV=production
      - DB_HOSTNAME=immich-database
      - DB_USERNAME=${IMMICH_DB_USERNAME}
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - DB_DATABASE_NAME=${IMMICH_DB_DATABASE_NAME}
      - REDIS_HOSTNAME=immich-redis
      - JWT_SECRET=${IMMICH_JWT_SECRET}
      - LOG_LEVEL=${IMMICH_LOG_LEVEL}
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-machine-learning:
    image: docker.io/altran1502/immich-machine-learning:release
    environment:
      - NODE_ENV=production
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-model-cache:/cache
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-redis:
    image: docker.io/redis:6.2-alpine
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-redis-data:/data

  immich-database:
    image: docker.io/tensorchord/pgvecto-rs:pg14-v0.1.11
    environment:
      - POSTGRES_USER=${IMMICH_DB_USERNAME}
      - POSTGRES_PASSWORD=${IMMICH_DB_PASSWORD}
      - POSTGRES_DB=${IMMICH_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-database-data:/var/lib/postgresql/data

  paperless-redis:
    image: docker.io/redis:7-alpine
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped
    volumes:
      - paperless-redis-data:/data

  paperless-gotenberg:
    image: docker.io/gotenberg/gotenberg:7.10
    command:
      - "gotenberg"
      - "--chromium-disable-javascript=true"
      - "--chromium-allow-list=file:///tmp/.*"
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped

  paperless-tika:
    image: docker.io/apache/tika:2.9.1.0
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped

  paperless:
    image: docker.io/paperlessngx/paperless-ngx:2.4.1
    depends_on:
      - paperless-redis
      - paperless-gotenberg
      - paperless-tika
    environment:
      - PAPERLESS_URL=https://paperless.${PROXY_PRIVATE_DOMAIN}
      - PAPERLESS_ADMIN_USER=${PAPERLESS_ADMIN_USER}
      - PAPERLESS_ADMIN_PASSWORD=${PAPERLESS_ADMIN_PASSWORD}
      - PAPERLESS_ALLOWED_HOSTS=localhost,paperless.${PROXY_PUBLIC_DOMAIN}
      - PAPERLESS_CSRF_TRUSTED_ORIGINS=https://paperless.${PROXY_PUBLIC_DOMAIN}
      - PAPERLESS_SECRET_KEY=${PAPERLESS_SECRET_KEY}
      - PAPERLESS_REDIS=redis://paperless-redis:6379
      - PAPERLESS_TIKA_ENABLED=true
      - PAPERLESS_TIKA_ENDPOINT=http://paperless-tika:9998
      - PAPERLESS_TIKA_GOTENBERG_ENDPOINT=http://paperless-gotenberg:3000
      - PAPERLESS_TIME_ZONE=${TIMEZONE}
      - USERMAP_UID=${MEDIA_PUID}
      - USERMAP_GID=${MEDIA_PGID}
      - TZ=${TIMEZONE}
    expose:
      - 8000 # webui
    healthcheck:
      test:
        ["CMD", "curl", "-fs", "-S", "--max-time", "2", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging: *logging
    networks:
      - proxy
      - paperless
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - paperless-data:/usr/src/paperless/data
      - paperless-media:/usr/src/paperless/media
      - ${PAPERLESS_CONSUME_PATH}:/usr/src/paperless/consume
      - ${PAPERLESS_EXPORT_PATH}:/usr/src/paperless/export

  vaultwarden:
    image: docker.io/vaultwarden/server:1.29.2-alpine
    environment:
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - DOMAIN=https://vaultwarden.${PROXY_PUBLIC_DOMAIN}
      - INVITATIONS_ALLOWED=false
      - ORG_CREATION_USERS=none
      - PASSWORD_HINTS_ALLOWED=false
      - SHOW_PASSWORD_HINT=false
      - SIGNUPS_ALLOWED=false
      - SMTP_FROM=${VAULTWARDEN_SMTP_FROM}
      - SMTP_HOST=${VAULTWARDEN_SMTP_HOST}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
      - SMTP_PORT=${VAULTWARDEN_SMTP_PORT}
      - SMTP_SECURITY=${VAULTWARDEN_SMTP_SECURITY}
      - SMTP_USERNAME=${VAULTWARDEN_SMTP_USERNAME}
      - TZ=${TIMEZONE}
      - YUBICO_CLIENT_ID=${VAULTWARDEN_YUBICO_CLIENT_ID}
      - YUBICO_SECRET_KEY=${VAULTWARDEN_YUBICO_SECRET_KEY}
    expose:
      - 80 # webui
      - 3012 # websocket
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - vaultwarden-data:/data

  gotify:
    image: docker.io/gotify/server:2.4.0
    environment:
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - gotify-data:/app/data

  diun:
    image: docker.io/crazymax/diun:4.26
    command: serve
    environment:
      - TZ=${TIMEZONE}
      - DIUN_NOTIF_GOTIFY_ENDPOINT=http://gotify
      - DIUN_NOTIF_GOTIFY_TOKEN=${MONITOR_DIUN_GOTIFY_TOKEN}
    logging: *logging
    networks:
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - diun-data:/data
      - ./diun/diun.yml:/diun.yml:ro
      - ./diun/config.yml:/config.yml:ro

  homeassistant:
    image: docker.io/homeassistant/home-assistant:stable
    environment:
      - TZ=${TIMEZONE}
    expose:
      - 8123 # webui
    logging: *logging
    networks:
      - proxy
      - mqtt
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /run/dbus:/run/dbus:ro
      - home-assistant-config:/config

  frigate:
    image: ghcr.io/blakeblackshear/frigate:stable
    devices:
      - ${FRIGATE_HARDWARE_RENDER}:/dev/dri/renderD128
    environment:
      - TZ=${TIMEZONE}
      - FRIGATE_MQTT_USER=${FRIGATE_MQTT_USER}
      - FRIGATE_MQTT_PASSWORD=${FRIGATE_MQTT_PASSWORD}
      - FRIGATE_CAMERA_USER=${FRIGATE_CAMERA_USER}
      - FRIGATE_CAMERA1_HOST=${FRIGATE_CAMERA1_HOST}
      - FRIGATE_CAMERA1_PASSWORD=${FRIGATE_CAMERA1_PASSWORD}
      - FRIGATE_CAMERA2_HOST=${FRIGATE_CAMERA2_HOST}
      - FRIGATE_CAMERA2_PASSWORD=${FRIGATE_CAMERA2_PASSWORD}
      - LIBVA_DRIVER_NAME=radeonsi
    expose:
      - 5000 # webui
      - 8554 # rtsp
      - 8855 # webrtc
    logging: *logging
    networks:
      - proxy
      - mqtt
    restart: unless-stopped
    shm_size: 128m
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${FRIGATE_STORAGE_PATH}:/media/frigate/recordings
      - ./frigate/config.yml:/config/config.yml
      - type: tmpfs
        target: /tmp/cache
        tmpfs:
          size: 1000000000

  mosquitto:
    image: docker.io/eclipse-mosquitto:2.0.18
    environment:
      - TZ=${TIMEZONE}
    expose:
      - 1883 # mqtt
    logging: *logging
    networks:
      - mqtt
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mosquitto-data:/mosquitto/data
      - ./mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf

  restic:
    image: docker.io/creativeprojects/resticprofile:latest
    command:
      - "-c"
      - "resticprofile schedule --all && crond -f"
    entrypoint: "/bin/sh"
    environment:
      - TZ=${TIMEZONE}
      - RESTIC_REPOSITORY_URL=${RESTIC_REPOSITORY_URL}
      - RESTIC_AWS_ACCESS_KEY_ID=${RESTIC_AWS_ACCESS_KEY_ID}
      - RESTIC_AWS_SECRET_ACCESS_KEY=${RESTIC_AWS_SECRET_ACCESS_KEY}
      - IMMICH_UPLOAD_LOCATION=${IMMICH_UPLOAD_LOCATION}
    hostname: box
    logging: *logging
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ./restic/profiles.yaml:/etc/resticprofile/profiles.yaml:ro
      - ./restic/password.txt:/etc/resticprofile/password.txt:ro
      - ${IMMICH_UPLOAD_LOCATION}:${IMMICH_UPLOAD_LOCATION}

  forgejo:
    image: codeberg.org/forgejo/forgejo:1.21-rootless
    depends_on:
      - forgejo-database
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - FORGEJO__database__DB_TYPE=postgres
      - FORGEJO__database__HOST=forgejo-database
      - FORGEJO__database__USER=${FORGEJO_DB_USERNAME}
      - FORGEJO__database__PASSWD=${FORGEJO_DB_PASSWORD}
      - FORGEJO__database__NAME=${FORGEJO_DB_DATABASE_NAME}
      - FORGEJO__server__DOMAIN=code.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__server__ROOT_URL=https://code.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__server__SSH_DOMAIN=git.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__service__DISABLE_REGISTRATION=true
    expose:
      - 3000 # webui
      - 2222 # ssh
    logging: *logging
    restart: unless-stopped
    networks:
      - proxy
      - forgejo
      - cloudflared
    volumes:
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
      - ./forgejo:/etc/gitea
      - forgejo-data:/data

  forgejo-database:
    image: docker.io/postgres:14-alpine
    environment:
      - POSTGRES_USER=${FORGEJO_DB_USERNAME}
      - POSTGRES_PASSWORD=${FORGEJO_DB_PASSWORD}
      - POSTGRES_DB=${FORGEJO_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - forgejo
    restart: unless-stopped
    volumes:
      - forgejo-database-data:/var/lib/postgresql/data

  # grafana:
  #   image: docker.io/grafana/grafana:10.1.4
  #   logging: *logging
  #   environment:
  #     - TZ=${TIMEZONE}
  #     - GF_SECURITY_ADMIN_USER=${MONITOR_GRAFANA_SECURITY_ADMIN_USER}
  #     - GF_SECURITY_ADMIN_PASSWORD=${MONITOR_GRAFANA_SECURITY_ADMIN_PASSWORD}
  #   networks:
  #     - proxy
  #     - monitor
  #   restart: unless-stopped
  #   volumes:
  #     - /etc/localtime:/etc/localtime:ro
  #     - grafana-data:/var/lib/grafana
  #     - grafana-config:/etc/grafana
  #     - ./grafana/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml
  #     - ./grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
  #
  # prometheus:
  #   image: docker.io/prom/prometheus:v2.45.0
  #   command:
  #     - "--config.file=/etc/prometheus/prometheus.yml"
  #     - "--storage.tsdb.path=/prometheus"
  #     - "--web.console.templates=/usr/share/prometheus/consoles"
  #     - "--web.console.libraries=/usr/share/prometheus/console_libraries"
  #   environment:
  #     - TZ=${TIMEZONE}
  #   logging: *logging
  #   networks:
  #     - monitor
  #   restart: unless-stopped
  #   volumes:
  #     - /etc/localtime:/etc/localtime:ro
  #     - prometheus-data:/prometheus
  #     - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
