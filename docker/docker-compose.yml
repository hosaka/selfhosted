version: "3"

x-logging: &logging
  driver: json-file
  options:
    max-size: 100m
    max-file: "3"
    tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"

x-hotio: &environment-hotio
  environment:
    - PUID=${MEDIA_PUID}
    - PGID=${MEDIA_PGID}
    - UMASK=${MEDIA_UMASK}
    - TZ=${TIMEZONE}

networks:
  proxy:
  cloudflared:
  qbit:
  immich:
  paperless:
  monitor:
  mqtt:
  forgejo:
  atuin:

volumes:
  caddy-config:

  jellyfin-config:
  radarr-config:
  sonarr-config:
  lidarr-config:
  navidrome-data:
  prowlarr-config:
  recyclarr-config:
  jellyseerr-config:
  sabnzbd-config:
  qbit-config:
  qbitmanage-config:
  frigate-config:

  immich-model-cache:
  immich-redis-data:
  immich-database-data:

  paperless-data:
  paperless-database-data:
  paperless-media:
  paperless-redis-data:

  atuin-database-data:
  vaultwarden-data:

  home-assistant-config:
  mosquitto-data:
  forgejo-data:
  forgejo-database-data:
  forgejo-runner-data:

  # monitoring
  gotify-data:
  diun-data:
  grafana-data:
  grafana-config:
  prometheus-data:

services:
  caddy:
    image: docker.io/hotio/caddy:release-2.7.6
    environment:
      - PROXY_CF_API_EMAIL=${PROXY_CF_API_EMAIL}
      - PROXY_CF_API_TOKEN=${PROXY_CF_API_TOKEN}
      - PROXY_PUBLIC_DOMAIN=${PROXY_PUBLIC_DOMAIN}
      - PROXY_PRIVATE_DOMAIN=${PROXY_PRIVATE_DOMAIN}
      - CADDY_INGRESS_NETWORKS=proxy
      - PUID=${MEDIA_PUID}
      - PGID=${MEDIA_PGID}
      - UMASK=${MEDIA_UMASK}
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
      - cloudflared
    ports:
      - 80:80
      - 443:443/tcp
      - 443:443/udp
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - caddy-config:/config
      - ./caddy/Caddyfile:/config/Caddyfile

  cloudflared:
    image: docker.io/cloudflare/cloudflared:2024.2.1
    command: tunnel --no-autoupdate run --token ${PROXY_CF_TUNNEL_TOKEN}
    extra_hosts:
      - host.docker.internal:host-gateway
    # expose:
    #   - 4506 # metrics
    logging: *logging
    networks:
      - cloudflared
      # - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro

  jellyfin:
    image: docker.io/hotio/jellyfin:release-10.8.13-1
    devices:
      - ${MEDIA_JELLYFIN_CARD}:/dev/dri/card0
      - ${MEDIA_JELLYFIN_RENDER}:/dev/dri/renderD128
    <<: *environment-hotio
    expose:
      - 8096 # webui
    group_add:
      - ${MEDIA_JELLYFIN_VIDEO_GROUP}
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_COLLECTION_PATH}:/data/media/collection
      - jellyfin-config:/config

  jellyseerr:
    image: docker.io/hotio/jellyseerr:release-1.7.0
    <<: *environment-hotio
    expose:
      - 5055 # webui
    logging: *logging
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - jellyseerr-config:/app/config

  radarr:
    image: docker.io/hotio/radarr:release-5.3.6.8612
    <<: *environment-hotio
    expose:
      - 7878 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - radarr-config:/config

  sonarr:
    image: docker.io/hotio/sonarr:release-4.0.1.929
    <<: *environment-hotio
    expose:
      - 8989 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - sonarr-config:/config

  prowlarr:
    image: docker.io/hotio/prowlarr:release-1.13.3.4273
    <<: *environment-hotio
    expose:
      - 9696 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${MEDIA_ROOT_PATH}:/data/media
      - prowlarr-config:/config

  recyclarr:
    image: docker.io/recyclarr/recyclarr:6.0.2
    environment:
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - recyclarr-config:/config
      - ./recyclarr/recyclarr.yml:/config/recyclarr.yml

  navidrome:
    image: docker.io/deluan/navidrome:0.51.0
    environment:
      - ND_ENABLETRANSCODINGCONFIG=false
    expose:
      - 4533 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - navidrome-data:/data
      - ${MEDIA_COLLECTION_MUSIC_PATH}:/music:ro

  lidarr:
    image: docker.io/hotio/lidarr:release-2.1.7.4030
    <<: *environment-hotio
    expose:
      - 8686 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - lidarr-config:/config
      - ${MEDIA_ROOT_PATH}:/data/media

  sabnzbd:
    image: docker.io/hotio/sabnzbd:release-4.2.2
    <<: *environment-hotio
    expose:
      - 8080 # webui
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - sabnzbd-config:/config
      - ${MEDIA_DOWNLOADS_USENET_PATH}:/data/media/downloads/usenet

  qbit:
    image: docker.io/hotio/qbittorrent:release-4.6.3
    cap_add:
      - NET_ADMIN
    environment:
      - PUID=${MEDIA_PUID}
      - PGID=${MEDIA_PGID}
      - UMASK=${MEDIA_UMASK}
      - TZ=${TIMEZONE}
      - VPN_ENABLED=${MEDIA_TORRENT_VPN_ENABLED}
      - VPN_IP_CHECK_DELAY=${MEDIA_TORRENT_VPN_IP_CHECK_DELAY}
      - VPN_IP_CHECK_EXIT=${MEDIA_TORRENT_VPN_IP_CHECK_EXIT}
      - VPN_LAN_NETWORK=${MEDIA_TORRENT_VPN_LAN_NETWORK}
    expose:
      - 8080 # webui
    logging: *logging
    networks:
      - proxy
      - qbit
    restart: unless-stopped
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=1
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - qbit-config:/config
      - ./qbittorrent/wg0.conf:/config/wireguard/wg0.conf:ro
      - ${MEDIA_DOWNLOADS_TORRENT_PATH}:/data/media/downloads/torrent

  qbitmanage:
    image: docker.io/hotio/qbitmanage:release-4.0.8
    <<: *environment-hotio
    logging: *logging
    networks:
      - qbit
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - qbitmanage-config:/config
      - ./qbittorrent/qbitmanage.yml:/config/config.yml
      - ${MEDIA_DOWNLOADS_TORRENT_PATH}:/data/media/downloads/torrent

  immich:
    image: docker.io/altran1502/immich-server:release
    depends_on:
      - immich-redis
      - immich-database
    entrypoint: ["/bin/sh", "./start-server.sh"]
    environment:
      - NODE_ENV=production
      - DB_HOSTNAME=immich-database
      - DB_USERNAME=${IMMICH_DB_USERNAME}
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - DB_DATABASE_NAME=${IMMICH_DB_DATABASE_NAME}
      - REDIS_HOSTNAME=immich-redis
      - JWT_SECRET=${IMMICH_JWT_SECRET}
      - LOG_LEVEL=${IMMICH_LOG_LEVEL}
      - TZ=${TIMEZONE}
    expose:
      - 3001 # webui
    logging: *logging
    networks:
      - proxy
      - immich
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-microservices:
    image: docker.io/altran1502/immich-server:release
    depends_on:
      - immich-redis
      - immich-database
    devices:
      - ${IMMICH_TRANSCODE_CARD}:/dev/dri/card0
    entrypoint: ["/bin/sh", "./start-microservices.sh"]
    environment:
      - NODE_ENV=production
      - DB_HOSTNAME=immich-database
      - DB_USERNAME=${IMMICH_DB_USERNAME}
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - DB_DATABASE_NAME=${IMMICH_DB_DATABASE_NAME}
      - REDIS_HOSTNAME=immich-redis
      - JWT_SECRET=${IMMICH_JWT_SECRET}
      - LOG_LEVEL=${IMMICH_LOG_LEVEL}
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-machine-learning:
    image: docker.io/altran1502/immich-machine-learning:release
    environment:
      - NODE_ENV=production
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-model-cache:/cache
      - ${IMMICH_UPLOAD_LOCATION}:/usr/src/app/upload

  immich-redis:
    image: docker.io/redis:6.2-alpine
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-redis-data:/data

  immich-database:
    image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0
    environment:
      - POSTGRES_USER=${IMMICH_DB_USERNAME}
      - POSTGRES_PASSWORD=${IMMICH_DB_PASSWORD}
      - POSTGRES_DB=${IMMICH_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - immich
    restart: unless-stopped
    volumes:
      - immich-database-data:/var/lib/postgresql/data

  paperless-redis:
    image: docker.io/redis:7-alpine
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped
    volumes:
      - paperless-redis-data:/data

  paperless-database:
    image: docker.io/postgres:14-alpine
    environment:
      - POSTGRES_USER=${PAPERLESS_DB_USERNAME}
      - POSTGRES_PASSWORD=${PAPERLESS_DB_PASSWORD}
      - POSTGRES_DB=${PAPERLESS_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped
    volumes:
      - paperless-database-data:/var/lib/postgresql/data

  paperless-gotenberg:
    image: docker.io/gotenberg/gotenberg:7.10
    command:
      - "gotenberg"
      - "--chromium-disable-javascript=true"
      - "--chromium-allow-list=file:///tmp/.*"
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped

  paperless-tika:
    image: docker.io/apache/tika:2.9.1.0
    logging: *logging
    networks:
      - paperless
    restart: unless-stopped

  paperless:
    image: docker.io/paperlessngx/paperless-ngx:2.5.3
    depends_on:
      - paperless-redis
      - paperless-database
      - paperless-gotenberg
      - paperless-tika
    environment:
      - PAPERLESS_URL=https://paperless.${PROXY_PRIVATE_DOMAIN}
      - PAPERLESS_ADMIN_USER=${PAPERLESS_ADMIN_USER}
      - PAPERLESS_ADMIN_PASSWORD=${PAPERLESS_ADMIN_PASSWORD}
      - PAPERLESS_ALLOWED_HOSTS=localhost,paperless.${PROXY_PUBLIC_DOMAIN}
      - PAPERLESS_CSRF_TRUSTED_ORIGINS=https://paperless.${PROXY_PUBLIC_DOMAIN}
      - PAPERLESS_SECRET_KEY=${PAPERLESS_SECRET_KEY}
      - PAPERLESS_REDIS=redis://paperless-redis:6379
      - PAPERLESS_DBHOST=paperless-database
      - PAPERLESS_DBUSER=${PAPERLESS_DB_USERNAME}
      - PAPERLESS_DBPASS=${PAPERLESS_DB_PASSWORD}
      - PAPERLESS_DBNAME=${PAPERLESS_DB_DATABASE_NAME}
      - PAPERLESS_TIKA_ENABLED=true
      - PAPERLESS_TIKA_ENDPOINT=http://paperless-tika:9998
      - PAPERLESS_TIKA_GOTENBERG_ENDPOINT=http://paperless-gotenberg:3000
      - PAPERLESS_TIME_ZONE=${TIMEZONE}
      - USERMAP_UID=${MEDIA_PUID}
      - USERMAP_GID=${MEDIA_PGID}
      - TZ=${TIMEZONE}
    expose:
      - 8000 # webui
    healthcheck:
      test:
        ["CMD", "curl", "-fs", "-S", "--max-time", "2", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging: *logging
    networks:
      - proxy
      - paperless
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - paperless-data:/usr/src/paperless/data
      - paperless-media:/usr/src/paperless/media
      - ${PAPERLESS_CONSUME_PATH}:/usr/src/paperless/consume
      - ${PAPERLESS_EXPORT_PATH}:/usr/src/paperless/export

  atuin-database:
    image: docker.io/postgres:14-alpine
    environment:
      - POSTGRES_USER=${ATUIN_DB_USERNAME}
      - POSTGRES_PASSWORD=${ATUIN_DB_PASSWORD}
      - POSTGRES_DB=${ATUIN_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - atuin
    restart: unless-stopped
    volumes:
      - atuin-database-data:/var/lib/postgresql/data

  atuin:
    image: ghcr.io/atuinsh/atuin:18.0.1
    command: server start
    depends_on:
      - atuin-database
    environment:
      - ATUIN_HOST=0.0.0.0
      - ATUIN_OPEN_REGISTRATION=false
      - ATUIN_DB_URI=postgres://${ATUIN_DB_USERNAME}:${ATUIN_DB_PASSWORD}@atuin-database/${ATUIN_DB_DATABASE_NAME}
      - TZ=${TIMEZONE}
    expose:
      - 8888 # api
    logging: *logging
    networks:
      - proxy
      - atuin
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro

  vaultwarden:
    image: docker.io/vaultwarden/server:1.30.2-alpine
    environment:
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - DOMAIN=https://vaultwarden.${PROXY_PUBLIC_DOMAIN}
      - INVITATIONS_ALLOWED=false
      - ORG_CREATION_USERS=none
      - PASSWORD_HINTS_ALLOWED=false
      - SHOW_PASSWORD_HINT=false
      - SIGNUPS_ALLOWED=false
      - SMTP_FROM=${VAULTWARDEN_SMTP_FROM}
      - SMTP_HOST=${VAULTWARDEN_SMTP_HOST}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
      - SMTP_PORT=${VAULTWARDEN_SMTP_PORT}
      - SMTP_SECURITY=${VAULTWARDEN_SMTP_SECURITY}
      - SMTP_USERNAME=${VAULTWARDEN_SMTP_USERNAME}
      - TZ=${TIMEZONE}
      - YUBICO_CLIENT_ID=${VAULTWARDEN_YUBICO_CLIENT_ID}
      - YUBICO_SECRET_KEY=${VAULTWARDEN_YUBICO_SECRET_KEY}
    expose:
      - 80 # webui
      - 3012 # websocket
    logging: *logging
    networks:
      - proxy
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - vaultwarden-data:/data

  gotify:
    image: docker.io/gotify/server:2.4.0
    environment:
      - TZ=${TIMEZONE}
    logging: *logging
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - gotify-data:/app/data

  diun:
    image: docker.io/crazymax/diun:4.26
    command: serve
    environment:
      - TZ=${TIMEZONE}
      - DIUN_NOTIF_GOTIFY_ENDPOINT=http://gotify
      - DIUN_NOTIF_GOTIFY_TOKEN=${MONITOR_DIUN_GOTIFY_TOKEN}
    logging: *logging
    networks:
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - diun-data:/data
      - ./diun/diun.yml:/diun.yml:ro
      - ./diun/config.yml:/config.yml:ro

  homeassistant:
    image: docker.io/homeassistant/home-assistant:2024.2
    environment:
      - TZ=${TIMEZONE}
    expose:
      - 8123 # webui
    logging: *logging
    networks:
      - proxy
      - mqtt
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /run/dbus:/run/dbus:ro
      - home-assistant-config:/config

  frigate:
    image: ghcr.io/blakeblackshear/frigate:0.13.1
    devices:
      - ${FRIGATE_HARDWARE_RENDER}:/dev/dri/renderD128
    environment:
      - TZ=${TIMEZONE}
      - FRIGATE_MQTT_USER=${FRIGATE_MQTT_USER}
      - FRIGATE_MQTT_PASSWORD=${FRIGATE_MQTT_PASSWORD}
      - FRIGATE_CAMERA_USER=${FRIGATE_CAMERA_USER}
      - FRIGATE_CAMERA1_HOST=${FRIGATE_CAMERA1_HOST}
      - FRIGATE_CAMERA1_PASSWORD=${FRIGATE_CAMERA1_PASSWORD}
      - FRIGATE_CAMERA2_HOST=${FRIGATE_CAMERA2_HOST}
      - FRIGATE_CAMERA2_PASSWORD=${FRIGATE_CAMERA2_PASSWORD}
      - LIBVA_DRIVER_NAME=radeonsi
    expose:
      - 5000 # webui
      - 8554 # rtsp
      - 8855 # webrtc
    logging: *logging
    networks:
      - proxy
      - mqtt
    restart: unless-stopped
    shm_size: 128m
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${FRIGATE_STORAGE_PATH}:/media/frigate/recordings
      - frigate-config:/config
      - ./frigate/config.yml:/config/config.yml
      - type: tmpfs
        target: /tmp/cache
        tmpfs:
          size: 1000000000

  mosquitto:
    image: docker.io/eclipse-mosquitto:2.0.18
    environment:
      - TZ=${TIMEZONE}
    expose:
      - 1883 # mqtt
    logging: *logging
    networks:
      - mqtt
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mosquitto-data:/mosquitto/data
      - ./mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf

  forgejo:
    # todo: automatically create a new user, part of the executed command:
    # forgejo admin user create --admin --username user --password password --email email@domain.tld
    image: codeberg.org/forgejo/forgejo:1.21-rootless
    command: >-
      bash -ec '
      gitea --config /etc/gitea/app.ini &
      sleep 10 ;
      forgejo forgejo-cli actions register --secret $FORGEJO_RUNNER_SECRET --name box ;
      sleep infinity ;
      '
    depends_on:
      - forgejo-database
    environment:
      - APP_NAME=Code
      - USER_UID=1000
      - USER_GID=1000
      - FORGEJO__database__DB_TYPE=postgres
      - FORGEJO__database__HOST=forgejo-database
      - FORGEJO__database__NAME=${FORGEJO_DB_DATABASE_NAME}
      - FORGEJO__database__PASSWD=${FORGEJO_DB_PASSWORD}
      - FORGEJO__database__USER=${FORGEJO_DB_USERNAME}
      - FORGEJO__git__PULL_REQUEST_PUSH_MESSAGE=false
      - FORGEJO__git__VERBOSE_PUSH=false
      - FORGEJO__openid__ENABLE_OPENID_SIGNUP=false
      - FORGEJO__security__INSTALL_LOCK=true
      - FORGEJO__security__INTERNAL_TOKEN=${FORGEJO_INTERNAL_TOKEN}
      # todo: this should only accept our reverse proxy
      # - FORGEJO__security__REVERSE_PROXY_TRUSTED_PROXIES=caddy
      - FORGEJO__security__SECRET_KEY=${FORGEJO_SECRET_KEY}
      - FORGEJO__server__DOMAIN=code.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__server__LFS_JWT_SECRET=${FORGEJO_LFS_JWT_SECRET}
      - FORGEJO__server__ROOT_URL=https://code.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__server__SSH_DOMAIN=git.${PROXY_PUBLIC_DOMAIN}
      - FORGEJO__server__SSH_LISTEN_PORT=2222
      - FORGEJO__server__SSH_PORT=22
      - FORGEJO__server__START_SSH_SERVER=true
      - FORGEJO__service__DEFAULT_KEEP_EMAIL_PRIVATE=true
      - FORGEJO__service__DISABLE_REGISTRATION=true
      - FORGEJO_RUNNER_SECRET=${FORGEJO_RUNNER_SECRET}
    expose:
      - 3000 # webui
      - 2222 # ssh
    logging: *logging
    restart: unless-stopped
    networks:
      - proxy
      - forgejo
      - cloudflared
    volumes:
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
      - forgejo-data:/var/lib/gitea

  forgejo-database:
    image: docker.io/postgres:14-alpine
    environment:
      - POSTGRES_USER=${FORGEJO_DB_USERNAME}
      - POSTGRES_PASSWORD=${FORGEJO_DB_PASSWORD}
      - POSTGRES_DB=${FORGEJO_DB_DATABASE_NAME}
    logging: *logging
    networks:
      - forgejo
    restart: unless-stopped
    volumes:
      - forgejo-database-data:/var/lib/postgresql/data

  forgejo-dind:
    image: docker.io/docker:dind
    command: ["dockerd", "-H", "tcp://0.0.0.0:2375", "--tls=false"]
    privileged: true
    logging: *logging
    networks:
      - forgejo

  forgejo-runner:
    image: code.forgejo.org/forgejo/runner:3.3.0
    command: >-
      bash -ec '
      while : ; do
        forgejo-runner create-runner-file --connect --instance http://forgejo:3000 --name box --secret $FORGEJO_RUNNER_SECRET && break ;
        sleep 1 ;
      done ;
      chown -R 1000:1000 /data ;
      forgejo-runner --config runner.yml daemon ;
      '
    depends_on:
      - forgejo
      - forgejo-dind
    environment:
      - FORGEJO_RUNNER_SECRET=${FORGEJO_RUNNER_SECRET}
    logging: *logging
    networks:
      - forgejo
    restart: unless-stopped
    volumes:
      - forgejo-runner-data:/data
      - ./forgejo/runner.yml:/data/runner.yml

  grafana:
    image: docker.io/grafana/grafana:10.3.1
    logging: *logging
    environment:
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_ANALYTICS_FEEDBACK_LINKS_ENABLED=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_DEFAULT_INSTANCE_NAME=selfhosted
      - GF_SECRETSCAN_ENABLED=false
      - GF_SECURITY_ADMIN_PASSWORD=${MONITOR_GRAFANA_SECURITY_ADMIN_PASSWORD}
      - GF_SECURITY_ADMIN_USER=${MONITOR_GRAFANA_SECURITY_ADMIN_USER}
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_SERVER_DOMAIN=grafana.${PROXY_PUBLIC_DOMAIN}
      - GF_SERVER_ROOT_URL=https://grafana.${PROXY_PUBLIC_DOMAIN}
      - GF_USERS_ALLOW_SIGN_UP=false
      - TZ=${TIMEZONE}
    expose:
      - 3000 # webui
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - grafana-data:/var/lib/grafana
      - grafana-config:/etc/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources

  prometheus:
    image: docker.io/prom/prometheus:v2.45.3
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.enable-lifecycle" # reload config on /-/reload POST
    environment:
      - TZ=${TIMEZONE}
    expose:
      - 9090 # metrics
    extra_hosts:
      - host.docker.internal:host-gateway
    logging: *logging
    networks:
      - proxy
      - monitor
    restart: unless-stopped
    volumes:
      - prometheus-data:/prometheus
      - ./prometheus/rules:/etc/prometheus/rules
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - /etc/localtime:/etc/localtime:ro

  node-exporter:
    image: docker.io/prom/node-exporter:v1.7.0
    command:
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--path.procfs=/host/proc"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker)($$|/)"
      - "--collector.netclass.ignored-devices=^(veth.*)$"
      - "--collector.netdev.device-exclude=^(veth.*)$"
      - "--collector.textfile.directory=/rootfs/home/restic/textfile_collector"
    expose:
      - 9100 # metrics
    logging: *logging
    network_mode: host
    pid: host
    restart: unless-stopped
    volumes:
      - /:/rootfs:ro
      - /sys:/host/sys:ro
      - /proc:/host/proc:ro
      - /etc/localtime:/etc/localtime:ro

  smartctl-exporter:
    image: docker.io/prometheuscommunity/smartctl-exporter:v0.12.0
    expose:
      - 9633 # metrics
    logging: *logging
    networks:
      - monitor
    privileged: true
    restart: unless-stopped
    user: root

  # cadvisor:
  #   image: gcr.io/cadvisor/cadvisor:0.49.1
